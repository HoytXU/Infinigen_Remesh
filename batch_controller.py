import os
import glob
import subprocess
import argparse
from multiprocessing import Pool

BASE_DIR = "/cpfs05/shared/landmark_3dgen/lvzhaoyang_group/shape2code/datasets/part2code/meshes"
VOXEL_SIZE = 0.005
TARGET_FACES = 50000
MAX_PROCESSES = 4 
CACHE_FILE = "relative_file_list.txt"

def get_tasks(limit=None):
    if os.path.exists(CACHE_FILE):
        print(f"ğŸ“„ Loading relative file list from '{CACHE_FILE}'...")
        with open(CACHE_FILE, "r") as f:
            relative_paths = [line.strip() for line in f]
    else:
        print(f"ğŸ“¦ Scanning directories under {BASE_DIR} ...")
        relative_paths = []
        factory_dirs = sorted(d for d in os.listdir(BASE_DIR) if os.path.isdir(os.path.join(BASE_DIR, d)))
        total = len(factory_dirs)

        for idx, factory in enumerate(factory_dirs):
            print(f"[{idx:02}/{total}] {factory}")
            factory_path = os.path.join(BASE_DIR, factory)
            samples = sorted(os.listdir(factory_path))

            for sample in samples:
                rel_path = os.path.join(factory, sample)
                abs_path = os.path.join(BASE_DIR, rel_path)
                if os.path.isfile(abs_path):
                    print(f"    â”œâ”€â”€ {sample} â†’ âœ… found")
                    relative_paths.append(rel_path)
                else:
                    print(f"    â”œâ”€â”€ {sample} â†’ âŒ missing")

        print(f"ğŸ’¾ Caching {len(relative_paths)} paths to '{CACHE_FILE}'...")
        with open(CACHE_FILE, "w") as f:
            for path in relative_paths:
                f.write(path + "\n")

    # ç”Ÿæˆä»»åŠ¡åˆ—è¡¨ï¼ˆè¾“å…¥è·¯å¾„ + è¾“å‡ºè·¯å¾„ï¼‰
    REMESH_DIR=BASE_DIR.rsplit("meshes", 1)[0]
    tasks = [
        (
            os.path.join(BASE_DIR, rel_path),
            os.path.join(REMESH_DIR, "remeshes", rel_path)
        )
        for rel_path in relative_paths
    ]

    if limit:
        tasks = tasks[:limit]
        print(f"ğŸ§ª Limiting to first {limit} tasks")

    print(f"âœ… Total collected tasks: {len(tasks)}\n")
    return tasks

def run_blender_remesh(task):
    input_path, output_path = task
    print(f"ğŸš€ [START] {input_path}")
    cmd = [
        "blender", "--background",
        "--python", "remesh_worker.py",
        "--", input_path, output_path, str(VOXEL_SIZE)
    ]
    try:
        result = subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        print(f"âœ… [DONE]  {output_path}")
    except subprocess.CalledProcessError as e:
        print(f"âŒ [FAIL]  {input_path}")
        print(f"    â†³ stderr: {e.stderr.decode(errors='ignore')[:200]}...")  # éƒ¨åˆ†æŠ¥é”™ä¿¡æ¯

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--max", type=int, help="Limit number of files to process")
    args = parser.parse_args()

    print("ğŸ“‹ Preparing remesh job list...\n")
    tasks = get_tasks(limit=args.max)

    if not tasks:
        print("âŒ No tasks to process. Exiting.")
        exit(1)

    print(f"ğŸ§µ Launching multiprocessing pool (workers = {MAX_PROCESSES})...\n")

    with Pool(processes=MAX_PROCESSES) as pool:
        pool.map(run_blender_remesh, tasks)

    print("\nğŸ‰ All tasks completed.")

